# Курс "Нейронные сети и глубокое обучение" Самарского университета  
Лектор [Артем Владимирович Никоноров](https://ssau.ru/staff/66320001-nikonorov-artem-vladimirovich), д.т.н., artniko@gmail.com  
Ассистенты: [Виктория Витальевна Евдокимова](https://ssau.ru/staff/304968209-evdokimova-viktoriya-vitalevna/edu), 
            [Никита Александрович Фирсов](https://ssau.ru/staff/441332557-firsov-nikita-aleksandrovich/edu) 

Телеграмм группа курса:
https://t.me/DL_SamU_2022

Курс основывается на предыдущих более обзорных лекциях и туториалах по глубокому обучению и его приложениях, в частности, вот [небольшая обзорная лекция](https://youtu.be/Gpq1PFUee88) в Кавказском Математическом Центре. Также во многом этот курс является адаптацией известнейшего курса http://cs231n.stanford.edu/  



## График проведения курса 2022-2023
**Видеозаписи лекций 2020-21 годов  можно найти по [ссылке](https://github.com/da0c/DL_Course_SamU).**  

Лекции раз в две недели по [пятницам](https://ssau.ru/rasp?staffId=66320001&selectedWeek=3&selectedWeekday=1).
Продолжительность лекции два астрономических часа.

Первая лекция: 10.09.22 в 10:00 в Zoom.


## Лекционный план 2022-2023  

**Лекция 1. Классификация, основанная на данных**   

Введение в курс.  
Задача классификации изображений.  
Подходы основанные на данных.  
Линейная классификация и knn-классификатор.  
  

**Лекция 2. Функции потерь и оптимизация.**  

Мультиклассовый SVM и его функция потерь.  
Софтмакс и мультимодальная логистическая регрессия.  
Оптимизация функции потерь.  
Стохастический градиентный спуск (SGD).  
  

**Лекция 3. Нейронные сети и обратное распространение ошибки.**  
 
Классификация с точки зрения нейронной сети.  
Многослойный перцептрон.  
Представление сети в виде вычислительного графа.
Алгоритм обратного распространения ошибки на вычислительном графе.  

**Лекция 4. Сверточные сети (СНС).**  
История.  
Основные операции СНС.  
Применение СНС вне задач машинного зрения.  

**Лекция 5. Инструментарий глубокого обучения.**  
CPU vs GPU vs TPU.  
Пакеты глубокого обучения, Tensorflow, Keras и другие.  
Вычислительные графы СНС.  

**Лекция 6. Обучение СНС, часть 1.**  

Активационные функции, обработка данных сетью.  
Пакетная нормализация и другие трюки.  
Transfer learning.

**Лекция 7. Обучение СНС, часть 2.**  

Политики обновления гиперпараметров.  
Тюнинг процесса обучения.
Аугментация данных.  

**Лекция 8. Архитектуры СНС**  

Базовые архитектуры - AlexNet, VGG, GoogleNet, ResNet, UNET и другие.  


**Лекция 9. Генеративные и рекуррентные модели**  


1. RNN/LSTM.  
Механизм attention.
Обработка естественного языка.

2. GAN сети.

3. Детектирование и сегментация.

**Лекция 10. Нейростевые модели и искусственный интеллект**  


## План лабораторных работ

- [для групп 6231, 6233](lab_schedule_6231_6233.md)
- [для группы 1143](lab_schedule_1143.md)



## Литература и дополнительные источники  

1. Отличная книга на русском по глубокому обучению -  
[С. И. Николенко, А. Кадурин, Е. В. Архангельская, Глубокое обучение. Погружение в мир нейронных сетей. 2018](https://www.ozon.ru/context/detail/id/154415719/)  
2. Отличная книга по техническим аспектам реализации на Python -  
[Шолле Франсуа, Глубокое обучение на Python](https://www.ozon.ru/context/detail/id/145615583/)  

3. [Лекционный курс К.В. Воронцова по машинному обучению](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%9A.%D0%92.%D0%92%D0%BE%D1%80%D0%BE%D0%BD%D1%86%D0%BE%D0%B2%29).
4. [Видеолекция академика Ю.И. Журавлева](https://www.youtube.com/watch?v=R3CMqrrIWOk) об истоках машинного обучения в СССР и о сочетании эвристики и науки в распознавании образов.  
5. Видеолекции С.И. Николенко по GAN сетям [1](https://www.youtube.com/watch?v=SlJgPIOlpiI), [2](https://www.youtube.com/watch?v=w38m5mTrG_M&t=1147s).
Хорошая проверка ваших знаний, на выходе из настоящего курса вы должны полностью понимать то, что говорится в этих лекциях по GAN.  





